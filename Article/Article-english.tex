\documentclass[twoside]{article}

\usepackage{sfuEng}


%%%%%%%%%%%%%%%%%%%%%  ЗДЕСЬ МОЖНО ПОДГРУЗИТЬ ДОПОЛНИТЕЛЬНЫЕ СТИЛЕВЫЕ ФАЙЛЫ, ЕСЛИ МАЛО УЖЕ ЗАГРУЖЕННЫХ %%%%%%%%%%%%%%%
\usepackage{} %%% и т.д.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%% ЭТА ЧАСТЬ ЗАПОЛНЯЕТСЯ РЕДАКЦИЕЙ  %%%%%%%%%%%%%%
\setcounter{page}{258}

\god{2009}

\nomer{2(3)}


\pp{258--270}                 %%%% Диапазон страниц

\firstvar{10.05.2009}         %%%% Дата получения статьи

\lastvar{10.06.2009}          %%%% Дата получения последнего варианта

\toprint{20.06.2009}          %%%% Дата принятия к печати

 %%%%%%%%%%%%%%%%%%%%%%%%%  ЭТА ЧАСТЬ ЗАПОЛНЯЕТСЯ АВТОРОМ %%%%%%%%%%%%%%%%%

\udk{512.74}

 %%%%%%%%%% Название статьи

\tit{Algebraic subgroups of the complex torus}


%%%%%%%%%%%%%%%%%% Краткая аннотация

\abstract{We study monomial parameterizations of~algebraic subgroups of~the~torus
over an~arbitrary field and separately over the~field of~complex numbers. It is proved that every
monomial parameterization defines an~algebraic group. The~necessary and sufficient conditions for
the~injectivity and existence of~such parameterizations are obtained.}

%%%%%%%%%%%%%%%%%% Ключевые слова
\keywords{algebraic subgroups,
monomial parameterization, complex algebraic torus.}



%%%%%%%%%% Название статьи на русском

\Rustit{Алгебраические подгруппы комплексного тора}

\authorRus{Николай А. Мишко}



%%%%%%%%%%%%%%%%%% Краткая аннотация на английском

\russianabstract{В~работе изучаются мономиальные параметризации алгебраических подгрупп тора
над произвольным полем и отдельно над полем комплексных чисел. Доказывается, что всякая
мономиальная параметризация определяет алгебраическую группу. Получены необходимые
и~достаточные условия инъективности и~существования такого рода параметризаций.}

%%%%%%%%%%%%%%%%%% Ключевые слова на английском

\russiankeywords{алгебраические подгруппы,
мономиальная параметризация, комплексный алгебраический тор.}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\maketitBegin %% DON'T CHANGE!!!
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Авторы, адреса рабочие, электронные адреса%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% Заполнять в скобках:\author{Имя}{Адрес}{e-mail} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% адрес разбивать на строки c помощью \\ (см. образец)

\author{Nikolay A. Mishko}
{Siberian Federal University,\\
Krasnoyarsk, Russia} {siegmentationfault@yandex.ru}





 \maketitEnd  %% DON'T CHANGE!!


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  ДАЛЕЕ ТЕКСТ СТАТЬИ АВТОРА %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\p}{\partial}
\renewcommand{\geq}{\geqslant}
\renewcommand{\leq}{\leqslant}
\renewcommand{\baselinestretch}{1}
\newtheorem {theorem}{Theorem}[section]
\newtheorem {rem}{Remark}[section]

\renewcommand{\thetheorem}{\arabic{section}.\arabic{theorem}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textit{An~algebraic subgroup} of a~group $G$ is a~subgroup endowed with an~algebraic variety structure,
i.e.~defined by~means of a~system of polynomial equations. A~natural example of the~algebraic group is a~set
of solutions of a~system of binomial equations:
$$
    z_1^{\alpha_1} z_2^{\alpha_2} \ldots z_n^{\alpha_n} = z_1^{\beta_1} z_2^{\beta_2} \ldots z_n^{\beta_n}.
$$

The~following theorem shows that such groups in~fact exhaust all algebraic varieties globally inheriting
the~group structure of the~torus $(K^\times)^n$.

For~vectors $\alpha = (\alpha_1, \ldots, \alpha_n) \in \mathbb{Z}^n$ and $z = (z_1, \ldots, z_n) \in G^n$ (where $G$ is~a~group)
we write $z^\alpha = z_1^{\alpha_1} \cdot z_2^{\alpha_2} \cdot \ldots \cdot z_n^{\alpha_n}$.

\medskip\noindent\textbf{Theorem} (Schmidt) \cite{Schm94}. \emph{Let $K$ be a~field. Every algebraic subgroup $H$
    of the~group $(K^{\times})^n$ is~defined by~a~system of~some number $N$ of binomial equations, namely,
    there are $N$ indices $\alpha_i, \beta_i \in \mathbb{Z}^n$ such that
    $$
        H = \{ z \in (K^{\times})^n\ |\ \forall 1 \leq i \leq N{:}\ z^{\alpha_i} = z^{\beta_i} \}.
    $$
}

Next we'll give a~self-contained (independent of~other major theorems) proof of~this theorem.
For~this purpose we need an~auxiliary statement.

\section*{1. Artin's theorem}
We denote the~set of homomorphisms between groups $G$ and $H$ by~$\mathrm{Hom}(G, H)$.

Let $G$ be a~group, $K$ be a~field, and $K^{\times}$ be its multiplicative group. Then an~arbitrary
homomorphism $f \in \mathrm{Hom}(G, K^{\times})$ is called \textit{a~character.} The~characters $f_1, f_2, \ldots, f_n$
are \textit{linearly independent} if
$$
    \forall \alpha_1, \alpha_2, \ldots, \alpha_n \in K{:}\ \alpha_1 f_1 + \alpha_2 f_2 + \ldots + \alpha_n f_n = 0 \Rightarrow \alpha_1 = \alpha_2 = \ldots = \alpha_n = 0.
$$

\noindent\textbf{Theorem} (Artin) \cite{Art48}.\emph{
    Any $n$ pairwise distinct characters are linearly independent.
}\medskip

\noindent{\it Proof.}
    Let us prove by induction on~the~number of characters $n$.

    Take an~arbitrary character $f$. Since it is a~homomorphism, $f(1_G) = 1$
    where $1_G$ is an~identity element in the~group $G$. But then if $\alpha f = 0$, then
    $\alpha = \alpha \cdot 1 = \alpha \cdot f(1_G) = 0$ what proves the~base case.

    Now let the~statement of theorem be true for any $n$ distinct characters. Let us prove
    it for $n + 1$ characters. Let $\alpha_1 f_1 + \alpha_2 f_2 + \ldots + \alpha_n f_n + \alpha_{n + 1} f_{n + 1} = 0$.

    Fix an~arbitrary $y \in G$. The for any $x \in G$ we have:
    \begin{equation}\label{Artin:first}
        \alpha_1 f_1(yx) + \alpha_2 f_2(yx) + \ldots + \alpha_n f_n(yx) + \alpha_{n + 1} f_{n + 1}(yx) = 0
    \end{equation}

    Since all $f_i$ are homomorphisms, $f_i(yx) = f_i(y) f_i(x)$, and therefore:
    $$
        \alpha_1 f_1(y) f_1(x) + \alpha_2 f_2(y) f_2(x) + \ldots + \alpha_n f_n(y) f_n(x) + \alpha_{n + 1} f_{n + 1}(y) f_{n + 1}(x) = 0
    $$

    On the~other hand, for $x \in G$ it is also true that:
    $$
        \alpha_1 f_1(x) + \alpha_2 f_2(x) + \ldots + \alpha_n f_n(x) + \alpha_{n + 1} f_{n + 1}(x) = 0
    $$

    Multiplying this equation by~$f_{n + 1}(y)$, we'll get:
    \begin{equation}\label{Artin:second}
        \alpha_1 f_{n + 1}(y) f_1(x) + \alpha_2 f_{n + 1}(y) f_2(x) + \ldots + \alpha_n f_{n + 1}(y) f_n(x) + \alpha_{n + 1} f_{n + 1}(y) f_{n + 1}(x) = 0
    \end{equation}

    Subtracting \eqref{Artin:first} from \eqref{Artin:second}, we obtain:
    $$
        \alpha_1 (f_{n + 1}(y) - f_1(y)) f_1(x) + \ldots + \alpha_n (f_{n + 1}(y) - f_n(y)) f_n(x) = 0
    $$

    $x$ was chosen arbitrarily, so we get a~linear combination of~$n$ characters:
    $$
        \alpha_1 (f_{n + 1}(y) - f_1(y)) f_1 + \ldots + \alpha_n (f_{n + 1}(y) - f_n(y)) f_n = 0
    $$

    But then, using the~inductive hypothesis, $\alpha_i (f_{n + 1}(y) - f_i(y)) = 0$. Now, choosing $y$
    for each $i = 1, \ldots, n$ such that $f_{n + 1}(y) \neq f_i(y)$ (it's always possible because all $f_i$
    are pairwise distinct), we obtain that $\alpha_1 = \alpha_2 = \ldots = \alpha_n = 0$.

    Thus, given the~above, $\alpha_{n + 1} f_{n + 1} = \alpha_1 f_1 + \alpha_2 f_2 + \ldots + \alpha_n f_n + \alpha_{n + 1} f_{n + 1} = 0$.
    According to~the~base case again, $\alpha_{n + 1} = 0$.
\hfill$\Box$

\section*{2. Proof of Schmidt's theorem}

\noindent{\it Proof.}
    Let $I \subseteq \mathbb{Z}^n$ be a~finite set of indices, and let $P_j(z) = \sum_{i \in I} a_{ji} z^i$ be the~$k$ polynomials
    defining the~subgroup:
    $$
        H = \{ z \in (K^{\times})^n\ |\ \forall 1 \leq j \leq k{:}\ P_j(z) = 0 \}.
    $$

    Since $z_1^{i} z_2^{i} = (z_1 z_2)^i$, the~mapping $z \mapsto z^i$ defines character $\chi_i \in \mathrm{Hom}(H, K^{\times})$.

    Consider an~equivalence relation $i \sim j \Leftrightarrow \chi_i = \chi_j$ on~the~set $I$. It partitions $I$
    into $m$ classes $I_1$, $I_2$, \textellipsis, $I_m$. Then for~all $i_1, i_2 \in I_j$ it is true that $\chi_{i_1} = \chi_{i_2}$.
    Let us denote this character corresponding to~each $I_k$ by~$\chi_k = \chi_{i_1} = \chi_{i_2}$.

    Combining like terms at~each~$\chi_k$ in the~polynomials $P_j$, we obtain:
    $$
        P_j = \sum_{i \in I} a_{ji} \chi_i = \sum_{k = 1}^{m} \left( \sum_{i \in I_k} a_{ji} \right) \chi_k.
    $$

    But $P_j$ vanishes on~the~whole $H$, so:
    $$
        \sum_{k = 1}^{m} \left( \sum_{i \in I_k} a_{ji} \right) \chi_k = 0.
    $$

    All $\chi_k$ are pairwise distinct by~their definition, so Artin's theorem applies. We conclude that:
    $$
    \sum_{i \in I_k} a_{ji} = 0.
    $$

    Finally, let $N_k$ be~the~cardinality of~$I_k$, $I_k = \{i_{k, 1}, i_{k, 2}, \ldots, i_{k, N_k}\}$, and $N = \sum_{k = 1}^m (N_k - 1)$.
    $I_k$ were taken such that the~characters $\chi_{i_{k, i}}$ and $\chi_{i_{k, j}}$ coincide on~$H$ for fixed $k$ and any $i$ and $j$.
    In~other words, this means that any element $z \in H$ satisfies the~equations $z^{i_{k, i}} = z^{i_{k, j}}$ for all $1 \leq i \leq N_k$
    and $1 \leq j \leq N_k$.

    Since the~equality is reflexive, symmetric and transitive, the~system of~all equations $z^{i_{k, i}} = z^{i_{k, j}}$
    is equivalent to~the~system of~$N$ equations composed of consecutive indices:
    $$
        z^{i_{1, 1}} = z^{i_{1, 2}}, z^{i_{1, 2}} = z^{i_{1, 3}}, \ldots, z^{i_{1, N_1 - 1}} = z^{i_{1, N_1}}, z^{i_{2, 1}} = z^{i_{2, 2}}, \ldots, z^{i_{m, N_m - 1}} = z^{i_{m, N_m}}.
    $$
    We denote the~set of solutions of~this system by~$A$.

    Let us show that $A$ coincides with $H$. Indeed, let $z \in H$. Then, as noted before,
    by~definition of~$I_k$ is is true that $z^{i_{k, j}} = \chi_k(z) = z^{i_{k, j + 1}}$;
    that is, $z \in A$ and $H \subseteq A$.

    Conversely, let $z \in A$. Then by~definition of~$A$ we have $z^{i_{k, j_1}} = z^{i_{k, j_1 + 1}} = \ldots = z^{i_{k, j_2 - 1}} = z^{i_{k, j_2}}$
    for all $1 \leq j_1 \leq j_2 \leq N_k$. Choose a~representative $i_k \in I_k$ in each~$I_k$
    and combine like terms:
    $$
        P_j(z) = \sum_{k = 1}^{m} \left( \sum_{i \in I_k} a_{ji} \right) z^{i_k} = \sum_{k = 1}^{m} 0 \cdot z^{i_k} = 0,
    $$
    what means $z \in H$, that is, $A \subseteq H$.

    Thus, $H = A$, so it suffices to take all the~consecutive indices from $I_k$ as $\alpha_i$ and $\beta_i$.
\hfill$\Box$\medskip

\section*{3. Injectivity of monomial parameterizations}

For~an abelian group $G$ and vectors $\alpha_1, \ldots, \alpha_n \in \mathbb{Z}^k$ consider the~mapping
$\phi_\alpha(t) = (t^{\alpha_1}, t^{\alpha_2}, \ldots, t^{\alpha_n})$ from~$G^k$ to~$G^n$ where $\alpha$ is~a~matrix
with rows $\alpha_i$. Since $(t_1 \cdot t_2)^{\alpha_i} = t_1^{\alpha_i} \cdot t_2^{\alpha_i}$,
$\phi_\alpha$ is a~homomorphism. Further we will that if $K$ is a~field, then the~mapping $\phi_\alpha$
for the~group $G = K^\times$ defines \textit{a~parameterization} of~some algebraic group.

\medskip\noindent\textbf{Proposition.}\emph{
    $\phi_{E} = 1_{G^n}$ where $E$ is an~$n \times n$ identity matrix, $1_X$ is~an~identity function on~$X$.
}

\medskip\noindent\textbf{Proposition.}\emph{
    $
        \forall \alpha \in \mathbb{Z}^{k \times m}, \beta \in \mathbb{Z}^{m \times n}{:}\ \phi_\alpha \circ \phi_\beta = \phi_{\alpha \beta}
    $.
}\medskip

\noindent{\it Proof.}
    Indeed, consider $t \in G^n$. Then the~$i$-th component of~$\phi_\alpha(\phi_\beta(t))$
    is equal to $\phi_\beta^{\alpha_i}(t) = (t^{\beta_1})^{\alpha_i^1} \ldots (t^{\beta_m})^{\alpha_i^m}
                                          = t_1^{\beta_1^1 \alpha_i^1 + \ldots + \beta_m^1 \alpha_i^m}
                                            \ldots
                                            t_n^{\beta_1^n \alpha_i^1 + \ldots + \beta_m^n \alpha_i^m}
                                          = t^{(\alpha \beta)_i}$.
\hfill$\Box$\medskip

Together these two propositions constitute a~condition for functoriality. More precisely, consider the~category $\mathrm{Matr}(R)$ of~matrices over the~ring $R$
whose objects are the~natural numbers and the~arrows between the~numbers $m$ and $n$ are the~matrices $R^{n \times m}$,
and the~category $\mathrm{Grp}$ of~small groups with small groups as objects and homomorphisms between them as arrows.
Then the~functor $\phi : \mathrm{Matr}(\mathbb{Z}) \rightarrow \mathrm{Grp}$ is defined:
$$
    \phi = \begin{cases}
        k \mapsto G^k; \\
        \alpha \mapsto \phi_\alpha.
    \end{cases}
$$

As lemma~2 shows, in fields of characteristic zero this functor is faithful.

\medskip\noindent\textbf{Lemma~1.}\emph{
    Let $K$ be a~field such that $\mathrm{char}(K) = 0$.
    Then $K^\times$ contains an~element of~infinite order.
}\medskip

\noindent{\it Proof.}
    Indeed, consider $2 = 1 + 1 \in K^\times$. Then for any $n > 0$:
    $$
        2^n - 1 = (1 + 1)^n - 1 = \sum_{k = 0}^n C^n_k 1^k 1^{n - k} - 1
                                = \sum_{k = 0}^n C^n_k \cdot 1 - 1
                                = \left(\sum_{k = 1}^n C^n_k \right) \cdot 1.
    $$

    On~the~right side we have the~sum of $\sum_{k = 1}^n C^n_k > 0$ units. Since $\mathrm{char}(K) = 0$,
    it is not equal to zero, that is, $2^n - 1 \neq 0$.
\hfill$\Box$\medskip

We denote \textit{a~standard basis} over $\mathbb{Z}^k$ by~$e_i$, that is, a~vector with one on~the~$i$-th place
and zeroes on~the~others. We denote the~$j$-th component of the~vector $\alpha_i$ by~$\alpha_i^j$
and the~vector consisting of the~$j$-th components by~$\alpha^j = (\alpha_1^j, \alpha_2^j, \ldots, \alpha_n^j)$.

\medskip\noindent\textbf{Lemma~2.}\emph{
    $
        \mathrm{char}(K) = 0 \Rightarrow \forall \alpha, \beta \in \mathbb{Z}^{n \times k}{:} \ \phi_\alpha = \phi_\beta \Leftrightarrow \alpha = \beta
    $.
}\medskip

\noindent{\it Proof.}
    The~left-to-right implication is obvious.

    Conversely, let $\phi_\alpha = \phi_\beta$. Using lemma~1 we fix an~element $z \in K^\times$
    of~infinite multiplicative order. Then for all $1 \leq i \leq n$ it is true that
    $$
        (z^{\alpha_1^i}, \ldots, z^{\alpha_k^i}) = \phi_\alpha(z \cdot e_i) = \phi_\beta(z \cdot e_i) = (z^{\beta_1^i}, \ldots, z^{\beta_k^i}).
    $$

    Thus, $z^{\alpha_j^i} = z^{\beta_j^i}$, that is, $z^{\alpha_j^i - \beta_j^i} = 1$.
    $z$ has infinite multiplicative order, so $\alpha_j^i - \beta_j^i = 0$.
\hfill$\Box$\medskip

We denote a~\textit{$\mathbb{Z}$-linear span} of~$\alpha_1$, $\ldots$, $\alpha_n$
by~$\mathrm{Span}_\mathbb{Z}(\alpha_1, \ldots, \alpha_n) = \{ k_1 \alpha_1 + \ldots + k_n \alpha_n \ | \ k_1,\allowbreak \ldots,\allowbreak k_n \in \mathbb{Z} \}$.
We say that $\alpha_1$, $\ldots$, $\alpha_n$ span the~whole lattice $\mathbb{Z}^k$ if
$\mathrm{Span}_\mathbb{Z}(\alpha_1, \ldots, \alpha_n) = \mathbb{Z}^k$.

It is easy to~obtain a~necessary condition for the~injectivity of the mapping $\phi_\alpha$. For this purpose,
we note that the~following proposition is obvious.

\medskip\noindent\textbf{Proposition.}\emph{
    $\mathrm{Span}_\mathbb{Z}(\alpha_1, \ldots, \alpha_n) = \mathbb{Z}^k \Leftrightarrow \forall i{:}\ e_i \in \mathrm{Span}_\mathbb{Z}(\alpha_1, \ldots, \alpha_n)$.
}

\medskip\noindent\textbf{Lemma~3.}\emph{
    If vectors $\alpha_i \in \mathbb{Z}^k$ span the~whole lattice $\mathbb{Z}^k$, then $\phi_\alpha$ is injective.
}\medskip

\noindent{\it Proof.}
    $\phi_\alpha$ is a~homomorphism, so it suffices to~show that $\ker(\phi_\alpha) = \{1\}$.
    We'll take $t \in G^k$ such that $\phi_\alpha(t) = 1$ and prove that $t = 1$.

    Indeed, since $\alpha_i$ span the~whole lattice, each $e_j$ can be expressed as their linear combination:
    $
        e_j = b^j_1 \alpha_1 + \ldots + b^j_n \alpha_n.
    $

    $\phi_\alpha(t) = 1$ means that $t^{\alpha_i} = 1$ for all $1 \leq i \leq n$.
    Fix $1 \leq j \leq k$ and raise to~the power of~$b^j_i$ both sides of~this equation:
    $t^{b^j_i \alpha_i} = (t^{\alpha_i})^{b^j_i} = 1^{b^j_i} = 1$.
    Finally, multiply the~obtained equations:
    $
        t_j = t^{e_j} = t^{b^j_1 \alpha_1 + \ldots + b^j_n \alpha_n} = t^{b^j_1 \alpha_1} \ldots t^{b^j_n \alpha_n} = 1 \cdot \ldots \cdot 1 = 1.
    $
\hfill$\Box$\medskip

Now we will show that the~obtained necessary condition is also sufficient for~the~group $G = {\mathbb{C}^\times}$.

We will use the~existence of \textit{Smith normal form} for~integer matrices \cite{Smth60}.
Let $\mathrm{diag}^{m \times n}_r(x_1,\allowbreak \ldots,\allowbreak x_r)$ be a~diagonal matrix $\mathrm{diag}(x_1,\allowbreak \ldots,\allowbreak x_r)$
augmented (or cut off) from the~bottom right by~zeroes to a~matrix of size $m \times n$.

\medskip\noindent\textbf{Theorem} (on~the~existence of Smith normal form).\emph{
    $$
        \forall \alpha \in \mathbb{Z}^{m \times n}{:}\
        \exists \beta_1 \in \mathrm{GL}^m(\mathbb{Z}), \beta_2 \in \mathrm{GL}^n(\mathbb{Z}){:} \
        \exists \varepsilon_1, \ldots, \varepsilon_r \in \mathbb{Z} \setminus \{0\}{:} \
        \beta_1 \alpha \beta_2 = \mathrm{diag}^{m \times n}_r(\varepsilon_1, \ldots, \varepsilon_r),
    $$
    where $\varepsilon_1 \mid \varepsilon_2 \mid \ldots \mid \varepsilon_r$ (the so~called invariant factors) and $r = \mathrm{rank}(\alpha)$.
}\medskip

Since the numbers $\varepsilon_1$, $\ldots$, $\varepsilon_r$ are chosen in a~unique way up to the~invertible element,
that is, up to $\pm 1$ in~case of $\mathbb{Z}$, we can always choose them positive. For them we denote
the~matrix $\mathrm{diag}^{m \times n}_r(\varepsilon_1, \ldots, \varepsilon_r)$ by~$\mathrm{SNF}(\alpha)$.

In~addition, since $\phi$ defines the~functor, it is clear that $\phi_\beta$ is bijective if $\beta \in \mathrm{GL}^n(\mathbb{Z})$.
Indeed, $\phi_\beta \circ \phi_{\beta^{-1}} = \phi_{\beta \beta^{-1}} = \phi_E = 1_{(K^\times)^n}$
and $\phi_{\beta^{-1}} \circ \phi_\beta = \phi_{\beta^{-1} \beta} = \phi_E = 1_{(K^\times)^n}$. Therefore,
the~following lemma holds.

\medskip\noindent\textbf{Lemma~4.}\emph{
    $\phi_\alpha : (\mathbb{C}^\times)^k \rightarrow (\mathbb{C}^\times)^n$ is injective if and only if
    $$
        \mathrm{SNF}(\alpha) = \mathrm{diag}^{n \times k}_k(1, 1, \ldots, 1).
    $$
}

\noindent{\it Proof.}
    Using the~theorem, let us take unimodular matrices $\beta_1 \in \mathrm{GL}^n(\mathbb{Z})$ and $\beta_2 \in \mathrm{GL}^k(\mathbb{Z})$
    such that $\beta_1 \alpha \beta_2 = \mathrm{SNF}(\alpha)$.
    Then, $\alpha = \beta_1^{-1} \mathrm{SNF}(\alpha) \beta_2^{-1}$ and $\phi_\alpha = \phi_{\beta_1^{-1} \mathrm{SNF}(\alpha) \beta_2^{-1}}
                                                                                    = \phi_{\beta_1^{-1}} \circ \phi_{\mathrm{SNF}(\alpha)} \circ \phi_{\beta_2^{-1}}$.

    Since, by~the~remark above, $\phi_{\beta_1^{-1}}$ and $\phi_{\beta_2^{-1}}$ are bijective, $\phi_\alpha$ is injective if and only if
    $\phi_{\mathrm{SNF(\alpha)}}$ is injective.

    $\phi_{\mathrm{SNF(\alpha)}}(t_1, \ldots, t_k) = (t_1^{\varepsilon_1}, \ldots, t_r^{\varepsilon_r}, 1, \ldots, 1)$, but $t \mapsto t^\varepsilon$
    is injective over $\mathbb{C}^\times$ only if $\varepsilon = \pm 1$ (otherwise $t^\varepsilon = t^\varepsilon \cdot 1 = t^\varepsilon \cdot u^\varepsilon = (tu)^\varepsilon$
    where $u \neq 1$ is~a~nontrivial $\varepsilon$-th root of~unity).
    By~choice of~signs, $\phi_{\mathrm{SNF(\alpha)}}$ is injective only if $\varepsilon_1 = \ldots = \varepsilon_r = 1$
    and $r = k \leq n$.
\hfill$\Box$\medskip

Let us formulate one more auxiliary theorem \cite{TsikhSad14}.

\medskip\noindent\textbf{Theorem~1.}\emph{
    Let $\alpha \in \mathbb{Z}^{n \times k}$. The~following statements are equivalent:
    \begin{enumerate}
        \item The~rows of the~matrix $\alpha$ span the~whole lattice $\mathbb{Z}^k$.
        \item The~maximal minors of the~matrix $\alpha$ are coprime.
        \item $\mathrm{SNF}(\alpha) = \mathrm{diag}^{n \times k}_k(1, \ldots, 1)$.
    \end{enumerate}
}

Thus, from lemma~4 and theorem~1 we obtain a~necessary and sufficient condition for~the~injectivity of~$\phi_\alpha$.

\medskip\noindent\textbf{Theorem.}\emph{
    $\phi_\alpha$ is injective over the~field $K = \mathbb{C}$ if and only if $\alpha_i$ span the~whole lattice $\mathbb{Z}^k$.
}

\section*{4. Monomial parameterizability of torus subgroups}

As in the~case of the~theory of curves and surfaces, we say that a~subgroup of an~algebraic torus
is \textit{parameterizable} if it can be expressed as an~image of some mapping. Similarly, a~subgroup
is \textit{monomially parameterizable} if it can be expressed as an~image of $\phi_\alpha$ for some matrix $\alpha$.

Since $\phi_\alpha$ is the~homomorphism, its image $\mathrm{Im}(\phi_\alpha)$ is a~subgroup in~$(K^\times)^n$.
Next we'll study two questions: whether $\mathrm{Im}(\phi_\alpha)$ is an~algebraic subgroup and whether
any algebraic subgroup is expressed by~some $\phi_\alpha$.

For this, first of all, we note that the~system of binomial equations $z^{\beta'_i} = z^{\beta''^i}$ is equivalent
to~the system $z^{\beta'_i - \beta''_i} = 1$, that is, exactly the~kernel of the~operator $\phi_{\beta'_i - \beta''_i}$;
therefore, the~first question is equivalent to whether a~given homomorphism $\phi_\alpha$ can be extended
to~the~exact sequence $(K^\times)^k \xrightarrow[]{\phi_\alpha} (K^\times)^n \xrightarrow[]{\phi_\beta} (K^\times)^m$.

\medskip\noindent\textbf{Lemma~5.}\emph{
    Let $G$ be a~group, $H$ be an~abelian group, $f \in \mathrm{Hom}(G, H)$, $g \in \mathrm{Hom}(H, G)$, and $g \circ f = 1_G$.
    Then the~sequence $G \xrightarrow[]{f} H \xrightarrow[]{f \circ g - 1_H} H$ is exact.
}\medskip

\noindent{\it Proof.}
    Indeed, $(f \circ g - 1_H) \circ f = f \circ g \circ f - f = f - f = 0$, that is, $\mathrm{Im}(f) \subseteq \ker(f \circ g - 1_H)$.
    Conversely, let $h \in \ker(f \circ g - 1_H)$. Then $f(g(h)) - h = 0 \Leftrightarrow h = f(g(h))$, that is, $h \in \mathrm{Im}(f)$
    and $\ker(f \circ g - 1_H) \subseteq \mathrm{Im}(f)$.
\hfill$\Box$\medskip

\medskip\noindent\textbf{Lemma~6.}\emph{
    $\phi_\alpha$ for $\alpha \in \mathbb{Z}^{n \times k}$ can be expressed as some image $\mathrm{Im}(\phi_\beta)$
    if the~mappings $g \mapsto g^{\varepsilon_i}$ are surjective in~$G$ for all $\varepsilon_i$ из~$\mathrm{SNF}(\alpha)$.
}\medskip

\noindent{\it Proof.}
    Let us again write $\alpha$ as $\alpha = \beta_1^{-1} \varepsilon \beta_2^{-1}$ where
    $
        \varepsilon = \mathrm{diag}^{n \times k}_r(\varepsilon_1, \ldots, \varepsilon_r).
    $

    Consider $\delta = \mathrm{diag}^{n \times r}_r(1, \ldots, 1)$ и~$\alpha' = \beta^{-1} \delta$.
    $\phi_{\alpha'}$ is injective since it is a~composition of injective functions. On~the~other hand, it is obvious that:
    $$
        \mathrm{Im}(\phi_\alpha) = \phi_{\beta_1^{-1}} (\phi_\varepsilon (\phi_{\beta_2^{-1}} (G^k)))
                                 = \phi_{\beta_1^{-1}} (\phi_\varepsilon (G^k))
                                 = \phi_{\beta_1^{-1}} (\phi_{\delta} (G^r))
                                 = \mathrm{Im}(\phi_{\alpha'}).
    $$

    It is easy to see that $\beta' = \delta^{\top} \beta$ is a~left inverse of the~matrix $\alpha'$,
    and hence $\phi_{\beta'}$ is a~left inverse of $\phi_{\alpha'}$. Since $(\phi_{\alpha'} \circ \phi_{\beta'}) \phi_{-E} = \phi_{\alpha' \beta' - E}$,
    we only need to~apply lemma~5.
\hfill$\Box$\medskip

\medskip\noindent\textbf{Theorem~2.}\emph{
    Any parameterization $\phi_\alpha$ for $\alpha \in \mathbb{Z}^{n \times k}$ defines an~algebraic subgroup
    of~$(K^\times)^n$ if the~field $K$ is algebraically closed.
}\medskip

The~answer to~the~second question is somewhat more complicated. For example, the~equation $z^n = 1$ defines an~algebraic
group for any $n \in \mathbb{Z}$ consisting of $n$~points on~$\mathbb{C}^\times$; but if $n \neq 0, \pm 1$
it is easy to see that it cannot be parameterized by~any $\phi_\alpha$.

Indeed, notice that the~group $\mathrm{Im}(\phi_\alpha)$ is isomorphic to~the~group $(\mathbb{C}^\times)^r$.
In~the~proof of lemma~6 we saw that for any $\phi_\alpha$ we can construct an~injective $\phi_{\alpha'} : (\mathbb{C}^\times)^r \rightarrow (\mathbb{C}^\times)^n$
such that $\mathrm{Im}(\phi_\alpha) = \mathrm{Im}(\phi_{\alpha'})$; but then $\phi_{\alpha'}$ defines the~desired isomorphism.
Thus, the~following proposition holds.

\medskip\noindent\textbf{Proposition.}\emph{
    For any matrix $\alpha \in \mathbb{Z}^{n \times k}$ the~group $\mathrm{Im}(\phi_\alpha)$ is isomorphic to an~algebraic torus
    of dimension at mosk $k$.
}\medskip

Moreover, since the~mapping $\phi_{\alpha'}$ is polynomial, it is, in particular, continuous in the~standard topology
on~$K = \mathbb{C}$. The~set $(\mathbb{C}^\times)^r$ is connected, and, as we know, the~continuous image
of a~connected set is connected, so next proposition is also true.

\medskip\noindent\textbf{Proposition.}\emph{
    $\mathrm{Im}(\phi_\alpha) \subseteq (\mathbb{C}^\times)^n$ is connected.
}\medskip

Now notice that the~group $z^n = 1$ is connected exactly when $n = 0, \pm 1$.
This indicates that the~criterion for the~existence of a~monomial parameterization
for~an~algebraic torus subgroup is connectedness.

We denote the~group of~$n$-th root of unity in the~field $K$ (given by~the~equation $z^n = 1$) by~$\omega_K(n) \subseteq K^\times$.
For~a~vector $x \in \mathbb{Z}^k$ we denote $\omega_K(x) = \omega_K(x_1) \times \ldots \times \omega_K(x_k) \subseteq (K^\times)^k$.
For brevity, we will write $\omega(x) = \omega_\mathbb{C}(x)$. It is known that the~group $\omega(x)$ is isomorphic
to~the~group $\mathbb{Z} / x_1 \mathbb{Z} \times \ldots \times \mathbb{Z} / x_k \mathbb{Z}$.

To~prove the~criterion described before, consider the~number $\Pi(\alpha) = |\omega_K(\varepsilon)|$
where $|G|$ is the~order of the~group $G$. $|G \times H| = |G| |H|$, so $\Pi(\alpha) = |\omega_K(\varepsilon_1)| \cdot \ldots \cdot |\omega_K(\varepsilon_r)|$.
Since $\omega(n)$ is isomorphic to~$\mathbb{Z} / n \mathbb{Z}$, in~case of the~field $K = \mathbb{C}$ it is also true that
$\Pi(\alpha) = \varepsilon_1 \cdot \ldots \cdot \varepsilon_r$.

\medskip\noindent\textbf{Lemma~7.}\emph{
    If $\Pi(\beta) = 1$, then there is $\alpha$ such that $\ker(\phi_\beta) = \mathrm{Im}(\phi_\alpha)$.
}\medskip

\noindent{\it Proof.}
    Let $H = \ker(\phi_\beta)$. Without loss of generality, we assume that the~rows of the~matrix $\beta$ are linearly independent
    over $\mathbb{Z}$ (it is clear that the~rows expressed as a~linear combination of the~other rows can be removed from the~matrix $\beta$
    without changing the~kernel).

    Let us write the~Smith normal form for $\beta$: $\beta = \beta_1^{-1} \varepsilon \beta_2^{-1}$.
    Since, according to~the~remark above, $\beta$ has full rank, the~matrix $\varepsilon$ has no zero rows.
    Furthermore, $\Pi(\beta) = 1$, so $|\omega_K(\varepsilon_i)| = 1$, that is, the~equations $z^{\varepsilon_i} = 1$
    have only $z = 1$ as a~solution.

    The~kernel of~$\varepsilon$ is given by~vectors of the~form $(0,\allowbreak \ldots,\allowbreak 0,\allowbreak t_{k + 1},\allowbreak \ldots, t_{n})$.
    Consider a~matrix $\delta \in \mathbb{Z}^{n \times (n - k)}$ corresponding to~the~linear operator
    $$
        (t_1, \ldots, t_{n - k}) \mapsto (0, \ldots, 0, t_1, \ldots, t_{n - k}).
    $$

    Let also $\alpha = \beta_2 \delta$. We'll prove that $\mathrm{Im}(\phi_\alpha) = \ker(\phi_\beta)$.

    Indeed, by~definition of~$\delta$ it holds that $\delta$, $\varepsilon \delta = 0$, so $\phi_{\beta} \circ \phi_{\alpha} = \phi_{\beta_1^{-1} \varepsilon \beta_2^{-1} \beta_2 \delta} = \phi_{\beta_1^{-1} \varepsilon \delta} = \phi_{0} = 1$,
    that is, $\mathrm{Im}(\phi_{\alpha}) \subseteq \ker(\phi_{\beta})$.

    Conversely, let $z \in \ker(\phi_\beta)$. Then $\phi_{\beta_1^{-1}} (\phi_{\varepsilon \beta_2^{-1}}(z)) = \phi_\beta(z) = 1$,
    so $\phi_{\varepsilon}(\phi_{\beta_2^{-1}}(z)) = \phi_{\varepsilon \beta_2^{-1}}(z) = \phi_{\beta_1}(1) = 1$.
    From the~form of the~matrix $\varepsilon$ we obtain that
    $
        \phi_{\beta_2^{-1}}(z) = (1,\allowbreak \ldots,\allowbreak 1,\allowbreak t_{k + 1},\allowbreak \ldots, t_n) = \phi_\delta(t_{k + 1}, \ldots, t_n)
    $; that is, $z = \phi_{\beta_2}(\phi_\delta(t_{k + 1}, \ldots, t_n)) = \phi_\alpha(t_{k + 1}, \ldots, t_n)$ for~some $t_j$.
    Thus, $z \in \mathrm{Im}(\phi_\alpha)$ and $\ker(\phi_\beta) \subseteq \mathrm{Im}(\phi_\alpha)$.
\hfill$\Box$\medskip

\medskip\noindent\textbf{Theorem~3.}\emph{
    The~number of connected components of $\ker(\phi_\beta) \subseteq ({\mathbb{C}^\times})^n$ is equal to~$\Pi(\beta)$.
}\medskip

\noindent{\it Proof.}
    Consider again the~Smith normal form of~$\beta$: $\beta$ = $\beta_1^{-1} \varepsilon \beta_2^{-1}$.
    $\phi_{\beta_1^{-1}}$ is isomorphism, so $\ker(\phi_{\beta}) = \ker(\phi_{\varepsilon \beta_2^{-1}})$.

    We denote the~rows of~$\beta_2^{-1}$ by~$b_i$. For a~vector $u \in \omega(\varepsilon)$ consider
    the~set $H_u = \{z \in ({\mathbb{C}^\times})^n \ | \ \forall 1 \leq i \leq r{:}\ z^{b_i} = u_i\}$.
    The~condition $\phi_\varepsilon(\phi_{\beta_2^{-1}}(z)) = 1$ is obviously equivalent to~the~condition
    $\exists u \in \omega(\varepsilon){:}\ z \in H_u$. Clearly, the~sets $H_u$ are disjunctive,
    so $\ker(\phi_{\beta})$ decomposes into a~disjunctive union:
    $
        \ker(\phi_{\beta}) = \bigsqcup_{u \in \omega(\varepsilon)} H_u.
    $
    There are exactly $\Pi(\beta)$ vectors $u \in \omega(\varepsilon)$, so it suffices to~show that
    each component $H_u$ is connected.

    Since, by~definition, $H_1 = \ker(\phi_{\delta \beta_2^{-1}})$ where $\delta = \mathrm{diag}^{k \times n}_r(1, \ldots, 1)$
    and $\Pi(\delta \beta_2^{-1}) = 1$, the~component $H_1$ is connected by~the~remark.

    Fix $u$ and consider the~matrix $\tau = \mathrm{diag}(1 / u_1, \ldots, 1 / u_r, 1, \ldots, 1)$
    and the~mapping $\psi = \phi_{\beta_2} \circ \phi_\tau \circ \phi_{\beta_2^{-1}}$.
    $\psi$ is a~continuous bijection, moreover, $\psi^{-1} = \phi_{\beta_2} \circ \phi_{\tau^{-1}} \circ \phi_{\beta_2^{-1}}$.

    By~definition, $\phi_{\beta_2^{-1}}(\psi(z)) = \phi_{\tau \beta_2^{-1}}(z)$. So if $z \in H_u$,
    then $\psi(z)^{b_i} = z^{b_i} / u_i = u_i / u_i = 1$. Conversely, if $z \in H_1$, then $\psi^{-1}(z)^{b_i} = u_i z^{b_i} = u_i$.
    Thus, $\psi(H_u) = H_1$, that is, $H_u$ is homeomorphic to~$H_1$, but $H_1$ is connected.
\hfill$\Box$\medskip

As we saw before, any algebraic subgroup $H$ of the~torus $(K^\times)^n$ can be expressed as $\ker(\phi_\beta)$
for some $\beta$. Notice that $\Pi(\beta)$ does not depend on~the~choice of~$\beta$ for~the~group $H$.

\medskip\noindent\textbf{Theorem~4.}\emph{
    If $\mathrm{char}(K) = 0$ and $\ker(\phi_{\beta}) = \ker(\phi_{\beta'})$, then $\Pi(\beta) = \Pi(\beta')$.
}\medskip

\noindent{\it Proof.}
    Let us write down the~Smith normal forms: $\beta = \beta_1^{-1} \varepsilon \beta_2^{-1}$
    and $\beta' = {\beta'}_1^{-1} \varepsilon' {\beta'}_2^{-1}$. So we have: $\ker(\phi_{\varepsilon \beta_2^{-1}}) = \ker(\phi_{\beta}) = \ker(\phi_{\beta'}) = \ker(\phi_{\varepsilon' {\beta'}_2^{-1}})$.

    Let $\varepsilon = \mathrm{diag}^{k \times n}_r(\varepsilon_1, \ldots, \varepsilon_r)$
    and $\varepsilon' = \mathrm{diag}^{k' \times n}_{r'}(\varepsilon'_1, \ldots, \varepsilon'_r)$.
    We write $\delta = \mathrm{diag}^{k \times n}_r(1, \ldots, 1)$ and $\delta' = \mathrm{diag}^{k' \times n}_{r'}(1, \ldots, 1)$.

    Since $\Pi(\delta \beta_2^{-1}) = \Pi(\delta' {\beta'}_2^{-1}) = 1$, both groups are parameterized
    according to~lemma~7 by~some $\phi_\alpha$ и~$\phi_{\alpha'}$ respectively.

    $\mathrm{Im}(\phi_\alpha) = \ker(\phi_{\delta \beta_2^{-1}}) \subseteq \ker(\phi_{\varepsilon \beta_2^{-1}}) = \ker(\phi_{\varepsilon' {\beta'}_2^{-1}})$,
    so $\phi_{\varepsilon' {\beta'}_2^{-1}} \circ \phi_\alpha = 1$. Using lemma~2, $\varepsilon' {\beta'}_2^{-1} \alpha = 0$.
    Multiplying both sides of the~equality by~the~matrix $\mathrm{diag}(1 / \varepsilon'_1, \ldots, 1 / \varepsilon'_r, 1, \ldots, 1)$
    one obtains that $\delta' {\beta'}_2^{-1} \alpha = 0$; but this means that $\ker(\phi_{\delta \beta_2^{-1}}) = \mathrm{Im}(\phi_\alpha) \subseteq \ker(\phi_{\delta' {\beta'}_2^{-1}})$.
    Similarly we get the~converse inclusion. Thus, $\ker(\phi_{\delta \beta_2^{-1}}) = \ker(\phi_{\delta' {\beta'}_2^{-1}})$.

    Consider the~quotient group $\ker(\phi_\beta) / \ker(\phi_{\delta \beta_2^{-1}}) = \ker(\phi_{\beta'}) / \ker(\phi_{\delta' {\beta'}_2^{-1}})$,
    also called \textit{the~component group.} The~mapping $\phi_{\delta \beta_2^{-1}} : \ker(\phi_\beta) \rightarrow \omega_K(\varepsilon) \times \{ 1 \} \times \ldots \times \{ 1 \}$
    induces an~injective homomorphism from the~quotient. In addtion, in the~theorem~3 the~bijection between the~components $H_u$
    was constructed (it generalises unchanged to~the~case of an~arbitrary field $K$), and hence they are all nonempty.
    This means that $\phi_{\delta \beta_2^{-1}}$ is surjective, so the~induced mapping is also surjective.
    The~reasoning is similar for~the~group $\omega_K(\varepsilon')$. Thus, we have a~chain of~isomorphisms:
    $$
        \omega(\varepsilon) \cong \ker(\phi_\beta) / \ker(\phi_{\delta \beta_2^{-1}}) = \ker(\phi_{\beta'}) / \ker(\phi_{\delta' {\beta'}_2^{-1}}) \cong \omega(\varepsilon').
    $$

    Isomorphic groups have the~same order, and hence $\Pi(\beta) = |\omega_K(\varepsilon)| = |\omega_K(\varepsilon')| = \Pi(\beta')$.
\hfill$\Box$\medskip

Finally, for any algebraic subgroup $H$ of the~torus we can define a~number $\Pi(H)$ equal to~$\Pi(\beta)$
for any $\beta$ such that $H = \ker(\phi_\beta)$.

Now for any algebraic subgroup $H \subseteq (\mathbb{C}^\times)^n$ we defined \textit{the~identity component} $H^\circ$
as a~connected component containing an~identity element of the~group $H$. It follows from theorem~3
that $\Pi(H^\circ) = 1$. Thus, the~following statements are proved.

\medskip\noindent\textbf{Theorem~5.}\emph{
    An~algebraic subgroup $H$ of the~torus $(K^\times)^n$ is parameterizable if and only if $\Pi(H) = 1$.
}

\medskip\noindent\textbf{Consequence.}\emph{
    An~algebraic subgroup $H$ of the~torus $(\mathbb{C}^\times)^n$ is parameterizable if and only if it is connected.
}

\medskip\noindent\textbf{Consequence.}\emph{
    Every algebraic subgroup $H$ of the~torus $(\mathbb{C}^\times)^n$ contains a~parameterizable subgroup $H^\circ$ of~the~same dimension.
}\medskip

\noindent{\it Proof.}
    All components of~$H$ are homeomorphic, what immediately proves the~theorem.
\hfill$\Box$\medskip

\section*{5. Linear independence over an~abelian group}

In~any abelian group $G$ there is naturally defined multiplication by~integers.
For a~vector $\alpha = (\alpha_1, \ldots, \alpha_k) \in \mathbb{Z}^k$ and element $g \in G$
by~$g \alpha$ we mean the~vector $(\alpha_1 g, \ldots, \alpha_k g) \in G^k$. We say that
the~collection of~vectors $\alpha_1, \ldots, \alpha_n \in \mathbb{Z}^k$ is \textit{linearly independent
over an~abelian group} if
$$
    \forall g_1, \ldots, g_n \in G{:}\ g_1 \alpha_1 + \ldots + g_n \alpha_n = 0 \Rightarrow g_1 = \ldots = g_n = 0.
$$

We see that the~linear independence of~vectors from $\mathbb{Z}^k$ over the~additive group of~the~field $\mathbb{R}$
is equivalent to~the~ordinary linear independence in the~vector space $\mathbb{R}^k$. However, for example,
$\mathbb{R} / 2 \pi \mathbb{Z}$, as it is known, has no ring structure, so it makes no sense
to~talk about the~$\mathbb{R} / 2 \pi \mathbb{Z}$-module $(\mathbb{R} / 2 \pi \mathbb{Z})^k$,
as well as about linear independence in it.

Let us immediately show how this definition is related to~the~studied parameterizations $\phi_\alpha$.

\medskip\noindent\textbf{Theorem~6.}\emph{
    $\phi_\alpha : G^k \rightarrow G^n$ is injective if and only if $\alpha^j$ are linearly independent over the~group $G$.
}\medskip

\noindent{\it Proof.} It follows directly from the~fact that injectivity of a~homomorphism is equivalent to the~triviality of~its kernel.
\hfill$\Box$\medskip

Next, we need two general lemmas.

\medskip\noindent\textbf{Lemma~8.}\emph{
    Let $G$ and $H$ be abelian groups. $\mu_1,\allowbreak \ldots,\allowbreak \mu_n \in \mathbb{Z}^k$ are linearly independent
    over $G \times H$ if and only if the are linearly independent over $G$ and over $H$.
}\medskip

\noindent{\it Proof.}
    The~elements of~$G \times H$ can be expressed as pairs $(g, h)$ where $g \in G$ and $h \in H$,
    so linear independence over $G \times H$ can be written as:
    $$
        \forall (g_1, h_1) \ldots, (g_n, h_n) \in G \times H{:}\ (g_1, h_1) \mu_1 + \ldots + (g_n, h_n) \mu_n = 0 \Rightarrow (g_1, h_1) = \ldots = (g_n, h_n) = 0.
    $$
    Furthermore, it is clear that
    $$
        (g_1, h_1) \mu_1 + \ldots + (g_n, h_n) \mu_n = 0 \Leftrightarrow g_1 \mu_1 + \ldots + g_n \mu_n = 0 \wedge h_1 \mu_1 + \ldots + h_n \mu_n = 0,
    $$
    as well as
    $$
        (g_1, h_1) = \ldots = (g_n, h_n) = 0 \Leftrightarrow g_1 = \ldots = g_n = 0 \wedge h_1 = \ldots = h_n = 0.
    $$

    By~fixing $g_1 = \ldots = g_n = 0$ and then $h_1 = \ldots = h_n = 0$, we'll obtain the~left-to-right
    implication. The~right-to-left implication is obvious.
\hfill$\Box$\medskip

\medskip\noindent\textbf{Lemma~9.}\emph{
    Let $G$ and $H$ be abelian groups, $f : G \rightarrow H$ be an~isomorphism. Then $\mu_1, \ldots, \mu_n \in \mathbb{Z}^k$
    are linearly independent over $G$ if and only if they are linearly independent over $H$.
}\medskip

\noindent{\it Proof.}
    Since $f$ is an~isomorphic, we obtain a~chain of equivalences:
    \begin{align*}
                       &\ \forall g_1, \ldots, g_n \in G{:}\ g_1 \alpha_1 + \ldots + g_n \alpha_n = 0 \Rightarrow g_1 = \ldots = g_n = 0 \\
        \Leftrightarrow&\ \forall g_1, \ldots, g_n \in G{:}\ f(g_1 \alpha_1 + \ldots + g_n \alpha_n) = 0 \Rightarrow g_1 = \ldots = g_n = 0 \\
        \Leftrightarrow&\ \forall g_1, \ldots, g_n \in G{:}\ f(g_1) \alpha_1 + \ldots + f(g_n) \alpha_n = 0 \Rightarrow f(g_1) = \ldots = f(g_n) = 0 \\
        \Leftrightarrow&\ \forall h_1, \ldots, h_n \in H{:}\ h_1 \alpha_1 + \ldots + h_n \alpha_n = 0 \Rightarrow h_1 = \ldots = h_n = 0.
        \tag*{$\Box$}
    \end{align*}
{}

Applying the~lemmas, we obtain the~injectivity criterion for~the~case $K = \mathbb{C}$.

\medskip\noindent\textbf{Consequence.}\emph{
    $\phi_\alpha$ is injective if and only if $\alpha^j$ are linearly independent over the~groups $\mathbb{R}$ and $\mathbb{R} / 2 \pi \mathbb{Z}$.
}\medskip

\noindent{\it Proof.}
    Let $S^1 = \{ z \in \mathbb{C}\ | \ |z| = 1 \}$ be a~circle group, a~subgroup in~${\mathbb{C}^\times}$.
    From the~trigonometric form $z = re^{i \theta}$ we see that the~group ${\mathbb{C}^\times}$ is isomorphic
    to the~product $\mathbb{R}_{> 0}^\times \times S^1$. The~mapping $t \mapsto e^t$ defines the~isomorphism
    of the~groups $\mathbb{R}$ and $\mathbb{R}_{> 0}^\times$, and the~mapping $\theta \mapsto e^{i\theta}$
    defines the~isomorphism between $\mathbb{R} / 2 \pi \mathbb{Z}$ and $S^1$; this proves a~corollary
    by~virtue of the~previous two lemmas.
\hfill$\Box$\medskip

\medskip\noindent\textbf{Consequence.}\emph{
    If $\phi_\alpha$ is injective over $K = \mathbb{C}$, then~$\mathrm{rank}(\alpha) = k$.
}\medskip

\noindent{\it Proof.}
    As noted before, linear independence over $\mathbb{R}$ is equivalent to linear independence
    in the~vector space $\mathbb{R}^k$ over $\mathbb{R}$, and this in turn is equivalent to having full rank.
\hfill$\Box$\medskip

In particular, this means that a~parameterization with the~$k > n$ variables is automatically
non-injective, which is to be expected.

Noticing that $\mathbb{R}_{>0}^\times \cong \mathbb{R}_{>0}^\times \times \mathbb{Z} / 1\mathbb{Z}$, $\mathbb{R}^\times \cong \mathbb{R}_{>0}^\times \times \mathbb{Z} / 2 \mathbb{Z}$
and $\mathbb{H}^\times \cong \mathbb{R}_{>0}^\times \times S^3$ (quaternions), it is easy to obtain similar conditions for~the~injectivity of~$\phi_\alpha$ over the~groups
$\mathbb{R}_{>0}^\times$, $\mathbb{R}^\times$, and $\mathbb{H}^\times$.

It seems natural that, since the~vectors $\alpha^j$ are integer-valued, their linear independence over $\mathbb{R}$
must reduce to~linear independence over $\mathbb{Z}$. Let us prove this.

\medskip\noindent\textbf{Lemma.}\emph{
    $\mu_1, \ldots, \mu_n \in \mathbb{Z}^k$ are linearly independent over $\mathbb{R}$ if and only if they are linearly independent over $\mathbb{Q}$.
}\medskip

\noindent{\it Proof.}
    The~left-to-right implication is obvious. Conversely, let $\mu_1, \ldots, \mu_n$ be linearly independent over $\mathbb{Q}$.
    Consider their linear combination: $r_1 \mu_1 + \ldots + r_n \mu_n = 0$.

    It is known that $\mathbb{R}$ is an~(infinite-dimensional) vector space over $\mathbb{Q}$, and any vector space has
    a~(Hamel) basis under assumption of the~axiom of~choice~\cite{Brbk70}. Using this, let us fix a~Hamel basis $B$ for $\mathbb{R}$
    over $\mathbb{Q}$. Let us decompose $r_i$ by this basis:
    $$
        r_i = q_i^1 b_1 + \ldots + q_i^s b_s
    $$
    where $q_i^j \in \mathbb{Q}$ and $b_j \in B$. There are finitely many vectors $r_i$,
    so the~set of basis vectors $b_j$ for them can be chosen to be the~same.
    Let us substitute the~decomposition into the~linear combination:
    \begin{align*}
        r_1 \mu_1 + \ldots + r_n \mu_n & = (q_1^1 b_1 + \ldots + q_1^s b_s) \mu_1 + \ldots + (q_n^1 b_1 + \ldots + q_n^s b_s) \mu_n \\
                                       & = (q_1^1 \mu_1 + \ldots + q_n^1 \mu_n) b_1 + \ldots + (q_1^s \mu_1 + \ldots + q_n^s \mu_n) b_s \\
                                       & = 0.
    \end{align*}

    In each coordinate we obtain a~rational linear combination of the~numbers $b_j$ equal to zero.
    By~virtue of linear independence, all these coordinates are equal to zero, so we have:
    $q_1^j \mu_1 + \ldots + q_n^j \mu_n = 0$.

    However, all $q_i^j$ are rational, and $\mu_i$ are linearly independent over $\mathbb{Q}$, so $q_i^j = 0$.
    Thus, $r_i = q_i^1 b_1 + \ldots + q_i^s b_s = 0 \cdot b_1 + \ldots + 0 \cdot b_s = 0$.
\hfill$\Box$\medskip

\medskip\noindent\textbf{Lemma.}\emph{
    $\mu_1, \ldots, \mu_n \in \mathbb{Z}^k$ are linearly independent over $\mathbb{Q}$ if and only if they are linearly independent over $\mathbb{Z}$.
}\medskip

\noindent{\it Proof.}
    The~left-to-right implication is again obvious. Conversely, let $q_1 \mu_1 + \ldots + q_n \mu_n = 0$ where $q_i \in \mathbb{Q}$.
    We choose the~common denominator $q \in \mathbb{N} \setminus \{ 0 \}$ for the~fractions $q_i$.
    We denote the~numerators by $p_i \in \mathbb{Z}$, that is, $q_i = p_i / q$.

    Then $(p_1 \mu_1 + \ldots + p_n \mu_n) / q = 0$, so $p_1 \mu_1 + \ldots + p_n \mu_n = 0$.
    All $p_i$ are integers, so using linear independence over $\mathbb{Z}$ we obtain that $p_1 = \ldots = p_n = 0$.
    Thus, $q_i = p_i / q = 0 / q = 0$.
\hfill$\Box$\medskip

\medskip\noindent\textbf{Consequence.}\emph{
    $\mu_1, \ldots, \mu_n \in \mathbb{Z}^k$ are linearly independent over $\mathbb{R}$ if and only if they are linearly independent over $\mathbb{Z}$.
}\medskip

Let us study in~more detail the~corollaries of linear independence over $\mathbb{R} / 2 \pi \mathbb{Z}$.

\medskip\noindent\textbf{Proposition.}\emph{
    $\mu_1, \ldots, \mu_n$ are linearly independent over $\mathbb{R} / 2 \pi \mathbb{Z}$
    if and only if they are linearly independent over $\mathbb{R} / \mathbb{Z}$.
}\medskip

\noindent{\it Proof.}
    This follows from that the~groups $\mathbb{R} / 2 \pi \mathbb{Z}$ and $\mathbb{R} / \mathbb{Z}$ are isomorphic.
\hfill$\Box$\medskip

For a~number $d \in \mathbb{Z}$ and a~vector $x \in \mathbb{Z}^k$ by $d \mid x$ we mean that $d \mid x_i$ for all $1 \leq i \leq k$
or, equivalently, $d \mid \gcd(x_1, \ldots, x_n)$.

\medskip\noindent\textbf{Lemma.}\emph{
    If $\mu_1, \ldots, \mu_n$ are linearly independent over $\mathbb{R} / \mathbb{Z}$, then:
    $$
        \forall d \in \mathbb{Z}{:}\ \forall x \in \mathbb{Z}^n{:}\ d \mid x_1 \mu_1 + \ldots + x_n \mu_n \Rightarrow d \mid x.
    $$
}

\noindent{\it Proof.}
    Indeed, consider $q_i = x_i / d \in \mathbb{R} / \mathbb{Z}$. The~divisibility of $x_1 \mu_1 + \ldots + x_n \mu_n$ by~$d$ means
    that in~$\mathbb{R} / \mathbb{Z}$ it holds that $(x_1 \mu_1 + \ldots + x_n \mu_n) / d = 0$, i.e. $q_1 \mu_1 + \ldots + q_n \mu_n = 0$.
    However, by virtue of linear independence, it is true that $q_1 = \ldots = q_n = 0$, but this means $d \mid x_i$ for all $1 \leq i \leq n$.
\hfill$\Box$\medskip

A~simple non-injectivity criterion follows immediately.

\medskip\noindent\textbf{Lemma.}\emph{
    If $\phi_\alpha$ is injective, then $\forall j{:}\ \gcd(\alpha^j) = 1$.
}

\medskip\noindent\textbf{Consequence.}\emph{
    If $\exists j{:}\ \gcd(\alpha^j) \neq 1$, then $\phi_\alpha$ is not an~injective mapping.
}\medskip

\noindent{\it Proof.}
    Choosing $x_j = 1$ and $x_1 = \ldots = x_{j - 1} = x_{j + 1} = \ldots = x_k = 0$, we obtain that:
    \begin{align*}
                         & \forall d \in \mathbb{Z}{:}\ \forall x \in \mathbb{Z}^k{:}\ d \mid x_1 \alpha^1 + \ldots + x_k \alpha^k \Rightarrow d \mid x \\
            \Rightarrow\ & \forall j{:}\ \forall d \in \mathbb{Z}{:}\ d \mid \alpha^j \Rightarrow d \mid 1
        \Leftrightarrow \forall j{:}\ \gcd(\alpha^j) = 1.
        \tag*{$\Box$}
    \end{align*}
{}

We denote the~set of prime numbers by $\mathbb{P} \subseteq \mathbb{Z}$.

\medskip\noindent\textbf{Lemma.}\emph{
    For arbitrary integer vectors $\mu_1, \ldots, \mu_n$ it is true that:
    \begin{align*}
                         & \forall d \in \mathbb{Z}{:}\ \forall x \in \mathbb{Z}^n{:}\ d \mid x_1 \mu_1 + \ldots + x_n \mu_n \Rightarrow d \mid x \\
        \Leftrightarrow\ & \forall p \in \mathbb{P}{:}\ \forall \beta \in \mathbb{N}{:}\ \forall x \in \mathbb{Z}^n{:}\ p^\beta \mid x_1 \mu_1 + \ldots + x_n \mu_n \Rightarrow p^\beta \mid x \\
        \Leftrightarrow\ & \forall p \in \mathbb{P}{:}\ \forall x \in \mathbb{Z}^n{:}\ p \mid x_1 \mu_1 + \ldots + x_n \mu_n \Rightarrow p \mid x
    \end{align*}
}

\noindent{\it Proof.}
    The left-to-right implications are obvious. Let us prove the~right-to-left ones.
    Fix an~arbitrary $d \in \mathbb{Z}$ and some vector $x \in \mathbb{Z}^k$.
    Factor $d$ into prime numbers: $d = p_1^{\beta_1} \ldots p_m^{\beta_m}$.

    Since $x_1 \mu_1 + \ldots + x_n \mu_n$ is divisible by~$d$, it is also divisible by every $p_i^{\beta_i}$.
    Using premise, we obtain that $p_i^{\beta_i} \mid x$ for all $1 \leq i \leq m$; but then
    $d = p_1^{\beta_1} \ldots p_m^{\beta_m} \mid x$ as required.

    Next, fix a~power $\beta \in \mathbb{N}$ and a~prime number $p$.
    Since $x_1 \mu_1 + \ldots + x_n \mu_n$ is divisible $p^\beta$,
    it is also divisible by~$p$, so it follow from the~premise that $p \mid x$.

    But this means that $x_i / p$ are integers, so $(x_1 / p) \mu_1 + \ldots + (x_n / p) \mu_n$ is divisible by~$p^{\beta - 1}$.
    Applying the~premise again, we obtain that $p \mid x / p$. Repeating this procedure $\beta$ times, we finally conclude
    that $p \mid x / p^{\beta - 1}$; but this is equivalent to~$p^\beta \mid x$.
\hfill$\Box$\medskip

The last condition can be rewritten as linear independence of vectors $\mu_i$ over the~fields $\mathbb{Z} / p\mathbb{Z}$
for all primes $p$, which is equivalent to having the~full rank for the~matrix $\mu$.

\medskip\noindent\textbf{Consequence.}\emph{
    For arbitrary integer vectors $\mu_1, \ldots, \mu_n$ it is true that:
    \begin{align*}
                         & \forall d \in \mathbb{Z}{:}\ \forall x \in \mathbb{Z}^n{:}\ d \mid x_1 \mu_1 + \ldots + x_n \mu_n \Rightarrow d \mid x \\
        \Leftrightarrow\ & \forall p \in \mathbb{P}{:}\ \forall x \in (\mathbb{Z} / p\mathbb{Z})^n{:}\ x_1 \mu_1 + \ldots + x_n \mu_n = 0 \Rightarrow x = 0 \\
        \Leftrightarrow\ & \forall p \in \mathbb{P}{:}\ \mathrm{rank}_{\mathbb{Z} / p \mathbb{Z}}(\mu) = \max(n, k).
    \end{align*}
}

Finally, we can re-prove the~already mentioned sufficient condition.

\medskip\noindent\textbf{Consequence.}\emph{
    If $\phi_\alpha$ is injective, then $\alpha_i$ span the~whole lattice.
}\medskip

\noindent{\it Proof.}
    Assume that $\alpha_i$ do not generate the~lattice. Then the~maximal minors of the~matrix $\alpha$ have a~common divisor $d > 1$.

    Take some prime divisor $p$ of the~number $d$. Since the~maximal minors are divisible by~$d$, they are also divisible by $p$;
    therefore in the~field $\mathbb{Z} / p \mathbb{Z}$ all maximal minors of $\alpha$ are equal to zero.
    So we have that $\mathrm{rank}_{\mathbb{Z} / p \mathbb{Z}}(\alpha) < \max(n, k)$~\cite{Brbk70},
    but this contradicts the~corollary.
\hfill$\Box$

\medskip

\emph{This work is supported by~the~Krasnoyarsk Mathematical Center and financed
by~the~Ministry of Science and Higher Education of the~Russian Federation (Agreement No. 075-02-2024-1429).}

\bigskip

\begin{thebibliography}{9}

\bibitem{Schm94} Schmidt, W.~M. Heights of points on subvarieties of $\mathbb{G}^n_m$~/ W.~M.~{Sch\-midt}~//
London Mathematical Society Lecture Note Series. Issue~235: Number Theory.
S\'eminaire de~th\'eorie des~nombres de~Paris 1993–94.~-- 1996.~-- P.~157--\allowbreak187.

\bibitem{Art48} Artin, Е. Galois Theory~/ E.~Artin; London~: University of~Notre Dame, 1942, 1944.~-- 96~p.~-- ISBN: 978-0-486-62342-9.

\bibitem{Smth60} Smith, H.J.S. On~systems of~linear indeterminate equations and {con\-gru\-ences}. Philos.~/ H.J.S.~Smith~//
Philosophical Transactions of the~Royal Society.~-- 1861.~-- Vol.~151.~-- P.~293--326.

\bibitem{TsikhSad14} Sadykov, T.~M. Multivariate Hypergeometric and Algebraic Functions~/ T.~M.~Sadykov, A.~K.~Tsikh;
Moscow~: Nauka, 2014.~-- 408~p.~-- ISBN: 978-5-02-039082-9.

\bibitem{Brbk70} Bourbaki, N. Alg\`ebre: Chapitres 1 \`a~3. \'El\'ements de~math\'ematique~/ N.~Bourbaki; Berlin~: Springer, 2006.~-- 652~p.~-- ISBN: 978-3-540-33849-9.

\end{thebibliography}

\makeRusTit   %% DON'T CHANGE!!!
\end{document}

